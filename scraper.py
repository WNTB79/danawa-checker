import asyncio
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
from datetime import datetime
import gspread
import json
import os

SH_ID = "1hKx0tg2jkaVswVIfkv8jbqx0QrlRkftFtjtVlR09cLQ" 
MAX_ROWS = 10000

URL_LIST = [
    "https://prod.danawa.com/info/?pcode=13412984", # 1ê°œì…
    "https://prod.danawa.com/info/?pcode=13413059", # 2ê°œì…
    "https://prod.danawa.com/info/?pcode=13413086", # 3ê°œì…
    "https://prod.danawa.com/info/?pcode=13413254", # 4ê°œì…
    "https://prod.danawa.com/info/?pcode=13678937", # 5ê°œì…
    "https://prod.danawa.com/info/?pcode=13413314"  # 6ê°œì…
]

async def get_danawa_data():
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, Korea) Chrome/122.0.0.0 Safari/537.36"
        )
        page = await context.new_page()
        
        now_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        final_matrix = [[now_str, f"{i}ìœ„"] for i in range(1, 6)]

        for idx, url in enumerate(URL_LIST, 1):
            try:
                print(f"ğŸš€ {idx}ê°œì… í˜ì´ì§€ ì ‘ì† ì¤‘ (ì˜¤ë¥¸ìª½ ì„¹ì…˜ ì¶”ì¶œ)...")
                await page.goto(url, wait_until="load", timeout=60000)
                await asyncio.sleep(8) 
                
                # ì˜¤ë¥¸ìª½ ì„¹ì…˜ì´ ë¡œë“œë˜ë„ë¡ í™•ì‹¤í•˜ê²Œ ìŠ¤í¬ë¡¤
                await page.evaluate("window.scrollTo(0, 1100)")
                await asyncio.sleep(2)

                content = await page.content()
                soup = BeautifulSoup(content, 'html.parser')
                
                # --- [í•µì‹¬ ìˆ˜ì •] ì˜¤ì§ ì˜¤ë¥¸ìª½ ì„¹ì…˜(#lowPrice_r) ì•ˆì— ìˆëŠ” ì•„ì´í…œë§Œ ê°€ì ¸ì˜µë‹ˆë‹¤ ---
                # ë§Œì•½ IDê°€ ì•ˆ ì¡í ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ 'ë°°ì†¡ë¹„ ìœ ë£Œ/ë¬´ë£Œ ì „ì²´' í´ë˜ìŠ¤ëª…ì„ ëª…ì‹œ
                right_area = soup.select("#lowPrice_r .diff_item")
                
                if not right_area:
                    # ë‹¤ë‚˜ì™€ ë ˆì´ì•„ì›ƒ ë³€í™” ëŒ€ì‘: ë¬´ë£Œë°°ì†¡ì´ ì•„ë‹Œ(not .free_delivery) ê°€ê²©ë¹„êµ ë¦¬ìŠ¤íŠ¸ íƒ€ê²ŸíŒ…
                    right_area = soup.select(".pay_comparison_list:not(.free_delivery) .diff_item")

                print(f"   ã„´ {idx}ê°œì… ì˜¤ë¥¸ìª½ ë°ì´í„° ë°œê²¬: {len(right_area)}ê±´")

                for i in range(5):
                    if i < len(right_area):
                        # í•´ë‹¹ ì•„ì´í…œ ë‚´ì˜ ê°€ê²© íƒœê·¸ë§Œ ì¶”ì¶œ
                        p_tag = right_area[i].select_one(".prc_c")
                        price = p_tag.get_text().replace(",", "").replace("ì›", "").strip() if p_tag else "0"
                        final_matrix[i].append(price)
                    else:
                        final_matrix[i].append("-")

            except Exception as e:
                print(f"âš ï¸ {idx}ê°œì… ìˆ˜ì§‘ ì—ëŸ¬: {e}")
                for i in range(5): final_matrix[i].append("-")

        # --- ë°ì´í„° ì €ì¥ ---
        # ì‹¤ì œ ê°€ê²© ë°ì´í„°ê°€ í•˜ë‚˜ë¼ë„ ë“¤ì–´ìˆëŠ”ì§€ í™•ì¸
        has_data = any(row[2] != "-" for row in final_matrix)
        
        if has_data:
            try:
                creds_raw = os.environ.get('GCP_CREDENTIALS', '').strip()
                creds = json.loads(creds_raw)
                gc = gspread.service_account_from_dict(creds)
                sh = gc.open_by_key(SH_ID)
                wks = sh.get_worksheet(0)
                
                wks.insert_rows(final_matrix, row=2)
                print(f"âœ… [ì„±ê³µ] ì˜¤ë¥¸ìª½ ì„¹ì…˜(ìœ /ë¬´ë£Œ ì „ì²´) ë°ì´í„° ì‚½ì… ì™„ë£Œ!")

                total_rows = len(wks.get_all_values())
                if total_rows > MAX_ROWS:
                    wks.delete_rows(MAX_ROWS + 1, total_rows)
            except Exception as e:
                print(f"âŒ ì‹œíŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
        else:
            print("âŒ ì˜¤ë¥¸ìª½ ì„¹ì…˜ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

        await browser.close()

if __name__ == "__main__":
    asyncio.run(get_danawa_data())
